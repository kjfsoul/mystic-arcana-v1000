#!/usr/bin/env python3
"""
Integration test script for data ingestion modules.
Demonstrates end-to-end ingestion of tarot, astrology, and psychology data.
"""
import sys
import os
import json
import logging
from datetime import datetime

# Add parent directory to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_ingestion_test.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

def test_web_scraping():
    """Test web scraping functionality for tarot, astrology, and psychology."""
    logger.info("Testing web scraping functionality...")
    
    try:
        from data_ingestion import scraper, utils
        
        # Test basic web scraper
        web_scraper = scraper.WebScraper()
        logger.info(f"WebScraper initialized with rate limit: {web_scraper.rate_limit_delay}s")
        
        # Test specialized scrapers
        tarot_scraper = scraper.TarotScraper()
        astrology_scraper = scraper.AstrologyScraper()
        psychology_scraper = scraper.PsychologyScraper()
        
        logger.info("Specialized scrapers initialized successfully")
        
        # Test tarot card scraping (simulated)
        logger.info("Testing tarot card data structure...")
        tarot_data = tarot_scraper.scrape_tarot_card("The Fool")
        logger.info(f"Tarot card data structure: {tarot_data}")
        
        # Test astrology sign scraping (simulated)
        logger.info("Testing astrology sign data structure...")
        astrology_data = astrology_scraper.scrape_zodiac_sign("Aries")
        logger.info(f"Astrology sign data structure: {astrology_data}")
        
        # Test psychology concept scraping (simulated)
        logger.info("Testing psychology concept data structure...")
        psychology_data = psychology_scraper.scrape_psychology_concept("Archetype")
        logger.info(f"Psychology concept data structure: {psychology_data}")
        
        # Save sample data
        sample_data = {
            'tarot': tarot_data,
            'astrology': astrology_data,
            'psychology': psychology_data,
            'timestamp': utils.format_timestamp(),
            'test_run': 'web_scraping'
        }
        
        output_file = 'sample_web_data.json'
        utils.save_json_data(sample_data, output_file)
        logger.info(f"Sample web data saved to {output_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"Web scraping test failed: {e}")
        return False

def test_youtube_processing():
    """Test YouTube transcript processing functionality."""
    logger.info("Testing YouTube processing functionality...")
    
    try:
        from data_ingestion import youtube_processor, utils
        
        # Test YouTube processor initialization
        youtube_proc = youtube_processor.YouTubeProcessor()
        logger.info("YouTubeProcessor initialized successfully")
        
        # Test available transcripts method (simulated)
        logger.info("Testing transcript availability checking...")
        available_transcripts = youtube_proc.get_available_transcripts("sample_video_id")
        logger.info(f"Available transcripts structure: {available_transcripts}")
        
        # Test video metadata method (simulated)
        logger.info("Testing video metadata structure...")
        metadata = youtube_proc.get_video_metadata("sample_video_id")
        logger.info(f"Video metadata structure: {metadata}")
        
        # Save sample data
        sample_data = {
            'transcripts': available_transcripts,
            'metadata': metadata,
            'timestamp': utils.format_timestamp(),
            'test_run': 'youtube_processing'
        }
        
        output_file = 'sample_youtube_data.json'
        utils.save_json_data(sample_data, output_file)
        logger.info(f"Sample YouTube data saved to {output_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"YouTube processing test failed: {e}")
        return False

def test_content_processing():
    """Test content processing functionality."""
    logger.info("Testing content processing functionality...")
    
    try:
        # Test content processor (will handle missing spaCy gracefully)
        try:
            from data_ingestion import content_processor
            processor = content_processor.ContentProcessor()
            logger.info("ContentProcessor initialized successfully")
            
            # Test text processing methods
            sample_text = "The Fool represents new beginnings, innocence, and potential. In astrology, Aries embodies similar energy."
            
            # Test text cleaning
            cleaned_text = processor.clean_text(sample_text)
            logger.info(f"Cleaned text: {cleaned_text}")
            
            # Test entity extraction (if spaCy available)
            entities = processor.extract_entities(sample_text)
            logger.info(f"Extracted entities: {len(entities)} found")
            
            # Test keyword extraction
            keywords = processor.extract_keywords(sample_text)
            logger.info(f"Extracted keywords: {keywords}")
            
            # Test summarization
            summary = processor.summarize(sample_text)
            logger.info(f"Generated summary: {summary}")
            
        except ImportError:
            logger.warning("spaCy not available - ContentProcessor tests skipped")
            # Create mock data for demonstration
            entities = []
            keywords = ["fool", "beginnings", "innocence", "aries", "energy"]
            summary = "The Fool represents new beginnings and potential, similar to Aries energy."
        
        # Save sample data
        sample_data = {
            'entities': entities,
            'keywords': keywords,
            'summary': summary,
            'timestamp': utils.format_timestamp(),
            'test_run': 'content_processing'
        }
        
        output_file = 'sample_content_data.json'
        from data_ingestion import utils
        utils.save_json_data(sample_data, output_file)
        logger.info(f"Sample content data saved to {output_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"Content processing test failed: {e}")
        return False

def test_data_validation():
    """Test data validation functionality."""
    logger.info("Testing data validation functionality...")
    
    try:
        from data_ingestion import data_validator, utils
        
        # Test data validator initialization
        validator = data_validator.DataValidator()
        logger.info("DataValidator initialized successfully")
        
        # Test content validation
        sample_content = {
            'title': 'The Fool - New Beginnings',
            'content': 'The Fool represents new beginnings, innocence, and potential in tarot readings.',
            'source': 'tarot_dictionary',
            'url': 'https://example.com/tarot/fool'
        }
        
        validation_result = validator.validate_content(sample_content)
        logger.info(f"Content validation result: {validation_result}")
        
        # Test duplicate checking
        existing_data = [sample_content]
        is_duplicate = validator.check_duplicates(sample_content, existing_data)
        logger.info(f"Duplicate check result: {is_duplicate}")
        
        # Test data cleaning
        dirty_content = {
            'title': '  The Fool\t\n',
            'content': 'The Fool represents new beginnings.\x00',
            'source': 'TAROT'
        }
        
        cleaned_content = validator.clean_and_normalize(dirty_content)
        logger.info(f"Cleaned content: {cleaned_content}")
        
        # Save sample data
        sample_data = {
            'validation_result': validation_result,
            'is_duplicate': is_duplicate,
            'cleaned_content': cleaned_content,
            'timestamp': utils.format_timestamp(),
            'test_run': 'data_validation'
        }
        
        output_file = 'sample_validation_data.json'
        utils.save_json_data(sample_data, output_file)
        logger.info(f"Sample validation data saved to {output_file}")
        
        return True
        
    except Exception as e:
        logger.error(f"Data validation test failed: {e}")
        return False

def test_error_handling():
    """Test error handling scenarios."""
    logger.info("Testing error handling scenarios...")
    
    try:
        from data_ingestion import scraper, utils
        
        # Test web scraper with invalid URL
        web_scraper = scraper.WebScraper()
        
        logger.info("Testing invalid URL handling...")
        invalid_result = web_scraper.scrape_url("not-a-valid-url")
        logger.info(f"Invalid URL result: {invalid_result}")
        
        # Test URL validation utility
        from data_ingestion.utils import is_valid_url, normalize_url
        
        valid_url = is_valid_url("https://www.example.com")
        invalid_url = is_valid_url("not-a-url")
        normalized = normalize_url("example.com/")
        
        logger.info(f"URL validation - valid: {valid_url}, invalid: {invalid_url}")
        logger.info(f"Normalized URL: {normalized}")
        
        # Test missing spaCy handling
        try:
            from data_ingestion import content_processor
            logger.info("spaCy is available")
        except ImportError:
            logger.info("spaCy is not available - graceful handling confirmed")
        
        return True
        
    except Exception as e:
        logger.error(f"Error handling test failed: {e}")
        return False

def main():
    """Run all integration tests."""
    logger.info("=" * 60)
    logger.info("DATA INGESTION INTEGRATION TEST")
    logger.info("=" * 60)
    
    start_time = datetime.now()
    logger.info(f"Test started at: {start_time}")
    
    # Run all tests
    tests = [
        ("Web Scraping", test_web_scraping),
        ("YouTube Processing", test_youtube_processing),
        ("Content Processing", test_content_processing),
        ("Data Validation", test_data_validation),
        ("Error Handling", test_error_handling)
    ]
    
    results = []
    for test_name, test_func in tests:
        logger.info(f"\n{'-' * 40}")
        logger.info(f"Running {test_name} test...")
        logger.info(f"{'-' * 40}")
        
        try:
            result = test_func()
            results.append((test_name, result))
            status = "PASSED" if result else "FAILED"
            logger.info(f"{test_name} test {status}")
        except Exception as e:
            logger.error(f"{test_name} test ERROR: {e}")
            results.append((test_name, False))
    
    # Summary
    end_time = datetime.now()
    duration = end_time - start_time
    
    logger.info(f"\n{'=' * 60}")
    logger.info("INTEGRATION TEST SUMMARY")
    logger.info(f"{'=' * 60}")
    logger.info(f"Start time: {start_time}")
    logger.info(f"End time: {end_time}")
    logger.info(f"Duration: {duration}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "‚úì PASSED" if result else "‚úó FAILED"
        logger.info(f"  {status} - {test_name}")
    
    logger.info(f"\nOverall Result: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("üéâ All integration tests PASSED!")
        logger.info("‚úÖ Data ingestion modules are ready for production use.")
    else:
        logger.warning("‚ö†Ô∏è  Some tests failed. Please review the errors above.")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
